{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook, we will train a CNN model to classify between the car images we gathered after validating the labels our algorithm generated "
      ],
      "metadata": {
        "id": "eYzDmf3r0JDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9X20GRekFm7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ],
      "metadata": {
        "id": "TgO6HF5DTjCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNaiireIkJtw",
        "outputId": "2aaed6ac-6cb8-4699-9f08-cd4be971b887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_car_data = np.load('/content/drive/MyDrive/Colab Notebooks/full_image_data.npy')\n",
        "original_car_labels = np.load('/content/drive/MyDrive/Colab Notebooks/full_label_data.npy')\n",
        "stanford_car_data = np.load('/content/drive/MyDrive/full_image_data_stanford.npy')\n",
        "stanford_car_labels = np.load('/content/drive/MyDrive/full_label_data_stanford.npy')\n",
        "berkley_car_data = np.load('/content/drive/MyDrive/Copy of full_image_data_berkley.npy')\n",
        "berkley_car_labels = np.load('/content/drive/MyDrive/Copy of full_label_data_berkley.npy')"
      ],
      "metadata": {
        "id": "IzpiDYaekMQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_car_data = list(original_car_data)\n",
        "stanford_car_data = list(stanford_car_data)\n",
        "berkley_car_data = list(berkley_car_data)"
      ],
      "metadata": {
        "id": "EPWdXzlbr9OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(original_car_data)):\n",
        "    original_car_data[i] = tf.image.resize(original_car_data[i], (128, 128))\n",
        "     \n",
        "for i in range(len(stanford_car_data)):\n",
        "    stanford_car_data[i] = tf.image.resize(stanford_car_data[i], (128, 128))\n",
        "\n",
        "for i in range(len(berkley_car_data)):\n",
        "    berkley_car_data[i] = tf.image.resize(berkley_car_data[i], (128, 128))\n",
        "    berkley_car_data[i] = berkley_car_data[i] / 255.0"
      ],
      "metadata": {
        "id": "26kzEi4brGvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_car_data = np.array(original_car_data)\n",
        "stanford_car_data = np.array(stanford_car_data)\n",
        "berkley_car_data = np.array(berkley_car_data)"
      ],
      "metadata": {
        "id": "_YRrdSiktAoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Since we have 8 classes to classify, we'll use stratified split to make sure a roughly equal amount of each class is in each set (training, validation and testing).\n",
        "\n",
        "## We're performing the split on each dataset sperately (original segmentation images, stanford cars dataset and pictures of cars from berkley deep drive) so that we can check performance across datasets, however training was performed on all datasets combined."
      ],
      "metadata": {
        "id": "WJi-gJQdbqh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate each of them train-test-validation\n",
        "\n",
        "# original_car_data:\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in sss.split(original_car_data, original_car_labels):  \n",
        "    X_train_original, X_test_original = original_car_data[train_index], original_car_data[test_index]\n",
        "    y_train_original, y_test_original = original_car_labels[train_index], original_car_labels[test_index]\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in sss.split(X_train_original, y_train_original):  \n",
        "    X_train_original, X_val_original = X_train_original[train_index], X_train_original[test_index]\n",
        "    y_train_original, y_val_original = y_train_original[train_index], y_train_original[test_index]\n",
        "\n",
        "print('Shape of original X_train is:', X_train_original.shape)\n",
        "print('Shape of original y_train is:', y_train_original.shape)\n",
        "print('Shape of original X_test is:', X_test_original.shape)\n",
        "print('Shape of original y_test is:', y_test_original.shape)\n",
        "print('Shape of original X_val is:', X_val_original.shape)\n",
        "print('Shape of original y_val is:', y_val_original.shape)\n",
        "print('\\n')\n",
        "\n",
        "# stanford car data:\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in sss.split(stanford_car_data, stanford_car_labels):  \n",
        "    X_train_stanford, X_test_stanford = stanford_car_data[train_index], stanford_car_data[test_index]\n",
        "    y_train_stanford, y_test_stanford = stanford_car_labels[train_index], stanford_car_labels[test_index]\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in sss.split(X_train_stanford, y_train_stanford):  \n",
        "    X_train_stanford, X_val_stanford = X_train_stanford[train_index], X_train_stanford[test_index]\n",
        "    y_train_stanford, y_val_stanford = y_train_stanford[train_index], y_train_stanford[test_index]\n",
        "\n",
        "print('Shape of stanford X_train is:', X_train_stanford.shape)\n",
        "print('Shape of stanford y_train is:', y_train_stanford.shape)\n",
        "print('Shape of stanford X_test is:', X_test_stanford.shape)\n",
        "print('Shape of stanford y_test is:', y_test_stanford.shape)\n",
        "print('Shape of stanford X_val is:', X_val_stanford.shape)\n",
        "print('Shape of stanford y_val is:', y_val_stanford.shape)\n",
        "print('\\n')\n",
        "\n",
        "# berkley car data:\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in sss.split(berkley_car_data, berkley_car_labels):  \n",
        "    X_train_berkley, X_test_berkley = berkley_car_data[train_index], berkley_car_data[test_index]\n",
        "    y_train_berkley, y_test_berkley = berkley_car_labels[train_index], berkley_car_labels[test_index]\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in sss.split(X_train_berkley, y_train_berkley):  \n",
        "    X_train_berkley, X_val_berkley = X_train_berkley[train_index], X_train_berkley[test_index]\n",
        "    y_train_berkley, y_val_berkley = y_train_berkley[train_index], y_train_berkley[test_index]\n",
        "\n",
        "print('Shape of berkley X_train is:', X_train_berkley.shape)\n",
        "print('Shape of berkley y_train is:', y_train_berkley.shape)\n",
        "print('Shape of berkley X_test is:', X_test_berkley.shape)\n",
        "print('Shape of berkley y_test is:', y_test_berkley.shape)\n",
        "print('Shape of berkley X_val is:', X_val_berkley.shape)\n",
        "print('Shape of berkley y_val is:', y_val_berkley.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0tkVdRG8krt",
        "outputId": "4fd0109b-5208-492f-c143-ea9110fd35f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of original X_train is: (318, 128, 128, 3)\n",
            "Shape of original y_train is: (318,)\n",
            "Shape of original X_test is: (100, 128, 128, 3)\n",
            "Shape of original y_test is: (100,)\n",
            "Shape of original X_val is: (80, 128, 128, 3)\n",
            "Shape of original y_val is: (80,)\n",
            "\n",
            "\n",
            "Shape of stanford X_train is: (260, 128, 128, 3)\n",
            "Shape of stanford y_train is: (260,)\n",
            "Shape of stanford X_test is: (82, 128, 128, 3)\n",
            "Shape of stanford y_test is: (82,)\n",
            "Shape of stanford X_val is: (65, 128, 128, 3)\n",
            "Shape of stanford y_val is: (65,)\n",
            "\n",
            "\n",
            "Shape of berkley X_train is: (329, 128, 128, 3)\n",
            "Shape of berkley y_train is: (329,)\n",
            "Shape of berkley X_test is: (103, 128, 128, 3)\n",
            "Shape of berkley y_test is: (103,)\n",
            "Shape of berkley X_val is: (83, 128, 128, 3)\n",
            "Shape of berkley y_val is: (83,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# concatenate all x_trains and all x_tests\n",
        "X_train = np.concatenate([X_train_original, X_train_stanford, X_train_berkley])\n",
        "X_test = np.concatenate([X_test_original, X_test_stanford, X_test_berkley])\n",
        "y_train = np.concatenate([y_train_original, y_train_stanford, y_train_berkley])\n",
        "y_test = np.concatenate([y_test_original, y_test_stanford, y_test_berkley])\n",
        "X_val = np.concatenate([X_val_original, X_val_stanford, X_val_berkley])\n",
        "y_val = np.concatenate([y_val_original, y_val_stanford, y_val_berkley])\n",
        "print('Shape of X_train is:', X_train.shape)\n",
        "print('Shape of X_test is:', X_test.shape)\n",
        "print('Shape of Y_train is:', y_train.shape)\n",
        "print('Shape of Y_test is:', y_test.shape)\n",
        "print('Shape of X_val is:', X_val.shape)\n",
        "print('Shape of Y_val is:', y_val.shape)\n"
      ],
      "metadata": {
        "id": "WrJVLW4k7kgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6d952d-be94-4444-caa2-9cca8fc733c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train is: (907, 128, 128, 3)\n",
            "Shape of X_test is: (285, 128, 128, 3)\n",
            "Shape of Y_train is: (907,)\n",
            "Shape of Y_test is: (285,)\n",
            "Shape of X_val is: (228, 128, 128, 3)\n",
            "Shape of Y_val is: (228,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_dict = {\n",
        "     0 : 'front',\n",
        "     1 : 'front-right',\n",
        "     2 : 'right',\n",
        "     3 : 'back-right',\n",
        "     4 : 'back' ,\n",
        "     5 : 'back-left',\n",
        "     6 : 'left' ,\n",
        "     7 : 'front-left'\n",
        "}\n",
        "rev_classification_dict = {v : k for k, v in classification_dict.items()}\n",
        "rev_classification_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr8nyc4VlCy8",
        "outputId": "2193294b-5ca7-484c-a81c-b2ea592b91ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'front': 0,\n",
              " 'front-right': 1,\n",
              " 'right': 2,\n",
              " 'back-right': 3,\n",
              " 'back': 4,\n",
              " 'back-left': 5,\n",
              " 'left': 6,\n",
              " 'front-left': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After alot of experimentation, we found that pretty heavy regularization improved validation set performance significantly. On most layers, BatchNorm performed better than Dropout, and around 25-35 epochs we're needed before overfitting. The best accuracy we managed to reach is around 78.95%."
      ],
      "metadata": {
        "id": "DawFjkGScYGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, AveragePooling2D, Dropout, BatchNormalization\n",
        "\n",
        "conv_model = Sequential()\n",
        "conv_model.add(Input(shape=(128, 128, 3)))\n",
        "\n",
        "conv_model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "conv_model.add(BatchNormalization())\n",
        "\n",
        "conv_model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "conv_model.add(BatchNormalization())\n",
        "\n",
        "conv_model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\")) \n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "conv_model.add(BatchNormalization())\n",
        "\n",
        "conv_model.add(Conv2D(256, kernel_size=(3, 3), activation=\"relu\")) \n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "conv_model.add(BatchNormalization())\n",
        "\n",
        "conv_model.add(Conv2D(512, kernel_size=(3, 3), activation=\"relu\")) \n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "\n",
        "conv_model.add(Flatten())\n",
        "conv_model.add(Dropout(0.5))\n",
        "conv_model.add(Dense(8, activation=\"softmax\"))\n",
        "\n",
        "conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "wB8tdTOVl5eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 10, epochs = 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMIrcgXf6wOB",
        "outputId": "ecfcc688-3214-4c1a-ad0c-6127f0773942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "91/91 [==============================] - 2s 15ms/step - loss: 5.3269 - accuracy: 0.2536 - val_loss: 2.6417 - val_accuracy: 0.1228\n",
            "Epoch 2/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 4.7448 - accuracy: 0.3363 - val_loss: 4.7744 - val_accuracy: 0.0658\n",
            "Epoch 3/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 4.3075 - accuracy: 0.3804 - val_loss: 3.8436 - val_accuracy: 0.1579\n",
            "Epoch 4/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 3.5436 - accuracy: 0.4598 - val_loss: 1.7974 - val_accuracy: 0.4868\n",
            "Epoch 5/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 2.7016 - accuracy: 0.5160 - val_loss: 2.2333 - val_accuracy: 0.4342\n",
            "Epoch 6/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 1.8607 - accuracy: 0.5843 - val_loss: 1.4045 - val_accuracy: 0.6096\n",
            "Epoch 7/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 1.3479 - accuracy: 0.6560 - val_loss: 1.0947 - val_accuracy: 0.6140\n",
            "Epoch 8/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 1.1374 - accuracy: 0.7078 - val_loss: 1.5939 - val_accuracy: 0.6009\n",
            "Epoch 9/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.8964 - accuracy: 0.7233 - val_loss: 1.1736 - val_accuracy: 0.6754\n",
            "Epoch 10/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.7778 - accuracy: 0.7652 - val_loss: 1.2101 - val_accuracy: 0.6798\n",
            "Epoch 11/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.6251 - accuracy: 0.7949 - val_loss: 1.0289 - val_accuracy: 0.6930\n",
            "Epoch 12/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.5642 - accuracy: 0.8115 - val_loss: 1.2886 - val_accuracy: 0.6798\n",
            "Epoch 13/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.4563 - accuracy: 0.8357 - val_loss: 1.0487 - val_accuracy: 0.7237\n",
            "Epoch 14/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.4601 - accuracy: 0.8423 - val_loss: 0.9401 - val_accuracy: 0.7061\n",
            "Epoch 15/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.3985 - accuracy: 0.8567 - val_loss: 0.9707 - val_accuracy: 0.7237\n",
            "Epoch 16/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.3621 - accuracy: 0.8732 - val_loss: 1.0013 - val_accuracy: 0.7149\n",
            "Epoch 17/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.3176 - accuracy: 0.8953 - val_loss: 0.9909 - val_accuracy: 0.7281\n",
            "Epoch 18/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.2849 - accuracy: 0.9052 - val_loss: 0.9477 - val_accuracy: 0.7412\n",
            "Epoch 19/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.2576 - accuracy: 0.9151 - val_loss: 0.9547 - val_accuracy: 0.7325\n",
            "Epoch 20/25\n",
            "91/91 [==============================] - 1s 11ms/step - loss: 0.2963 - accuracy: 0.8942 - val_loss: 1.1697 - val_accuracy: 0.7149\n",
            "Epoch 21/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.3201 - accuracy: 0.8897 - val_loss: 1.1561 - val_accuracy: 0.7018\n",
            "Epoch 22/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.3356 - accuracy: 0.8986 - val_loss: 1.4124 - val_accuracy: 0.6798\n",
            "Epoch 23/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.3039 - accuracy: 0.8908 - val_loss: 1.0594 - val_accuracy: 0.7368\n",
            "Epoch 24/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.3128 - accuracy: 0.8920 - val_loss: 1.1622 - val_accuracy: 0.7325\n",
            "Epoch 25/25\n",
            "91/91 [==============================] - 1s 12ms/step - loss: 0.3045 - accuracy: 0.8997 - val_loss: 0.9319 - val_accuracy: 0.7675\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0f2c53990>"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We'll save the final model and check it's overall results in the DL Project model results notebook."
      ],
      "metadata": {
        "id": "b2zjMDZVX3nV"
      }
    }
  ]
}